{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "# read local .env file\n",
    "# find_dotenv 函数通常用于搜索和定位目录树中的 .env 文件，而 load_dotenv 函数用于将 .env 文件中的变量加载到环境中。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "# embeddings = OpenAIEmbeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:26.681474900Z",
     "start_time": "2024-01-28T14:34:26.419602900Z"
    }
   },
   "id": "77de509f7263c90a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "该教程涵盖了如何将链序列化到磁盘并从磁盘反序列化。我们使用的序列化格式是json或yaml。目前仅有一些链支持此类型的序列化。我们将逐步增加支持的链数。\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "template = \"\"\"Question: {question}\n",
    " \n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:26.773299700Z",
     "start_time": "2024-01-28T14:34:26.767299500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "llm_chain.save(\"files/llm_chain.json\")\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:27.326094400Z",
     "start_time": "2024-01-28T14:34:27.305094900Z"
    }
   },
   "id": "bdef4fb78e74ae8d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n{\\n    \"name\": null,\\n    \"memory\": null,\\n    \"verbose\": true,\\n    \"tags\": null,\\n    \"metadata\": null,\\n    \"prompt\": {\\n        \"name\": null,\\n        \"input_variables\": [\\n            \"question\"\\n        ],\\n        \"input_types\": {},\\n        \"output_parser\": null,\\n        \"partial_variables\": {},\\n        \"template\": \"Question: {question}\\n \\nAnswer: Let\\'s think step by step.\",\\n        \"template_format\": \"f-string\",\\n        \"validate_template\": false,\\n        \"_type\": \"prompt\"\\n    },\\n    \"llm\": {\\n        \"model_name\": \"gpt-3.5-turbo\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"stream\": false,\\n        \"n\": 1,\\n        \"temperature\": 0.3,\\n        \"_type\": \"openai-chat\"\\n    },\\n    \"output_key\": \"text\",\\n    \"output_parser\": {\\n        \"name\": null,\\n        \"_type\": \"default\"\\n    },\\n    \"return_final_only\": true,\\n    \"llm_kwargs\": {},\\n    \"_type\": \"llm_chain\"\\n}\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cat files/llm_chain.json\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "    \"name\": null,\n",
    "    \"memory\": null,\n",
    "    \"verbose\": true,\n",
    "    \"tags\": null,\n",
    "    \"metadata\": null,\n",
    "    \"prompt\": {\n",
    "        \"name\": null,\n",
    "        \"input_variables\": [\n",
    "            \"question\"\n",
    "        ],\n",
    "        \"input_types\": {},\n",
    "        \"output_parser\": null,\n",
    "        \"partial_variables\": {},\n",
    "        \"template\": \"Question: {question}\\n \\nAnswer: Let's think step by step.\",\n",
    "        \"template_format\": \"f-string\",\n",
    "        \"validate_template\": false,\n",
    "        \"_type\": \"prompt\"\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"stream\": false,\n",
    "        \"n\": 1,\n",
    "        \"temperature\": 0.3,\n",
    "        \"_type\": \"openai-chat\"\n",
    "    },\n",
    "    \"output_key\": \"text\",\n",
    "    \"output_parser\": {\n",
    "        \"name\": null,\n",
    "        \"_type\": \"default\"\n",
    "    },\n",
    "    \"return_final_only\": true,\n",
    "    \"llm_kwargs\": {},\n",
    "    \"_type\": \"llm_chain\"\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:27.843959100Z",
     "start_time": "2024-01-28T14:34:27.828561400Z"
    }
   },
   "id": "7ef16a45d1ee9a5a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading openai-chat LLM not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 9\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_chain\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m chain \u001B[38;5;241m=\u001B[39m \u001B[43mload_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfiles/llm_chain.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\loading.py:596\u001B[0m, in \u001B[0;36mload_chain\u001B[1;34m(path, **kwargs)\u001B[0m\n\u001B[0;32m    594\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hub_result\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_chain_from_file(path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\loading.py:623\u001B[0m, in \u001B[0;36m_load_chain_from_file\u001B[1;34m(file, **kwargs)\u001B[0m\n\u001B[0;32m    620\u001B[0m     config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;66;03m# Load the chain from the config now.\u001B[39;00m\n\u001B[1;32m--> 623\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m load_chain_from_config(config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\loading.py:586\u001B[0m, in \u001B[0;36mload_chain_from_config\u001B[1;34m(config, **kwargs)\u001B[0m\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m chain not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    585\u001B[0m chain_loader \u001B[38;5;241m=\u001B[39m type_to_loader_dict[config_type]\n\u001B[1;32m--> 586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chain_loader(config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\loading.py:40\u001B[0m, in \u001B[0;36m_load_llm_chain\u001B[1;34m(config, **kwargs)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config:\n\u001B[0;32m     39\u001B[0m     llm_config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m     llm \u001B[38;5;241m=\u001B[39m \u001B[43mload_llm_from_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm_path\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config:\n\u001B[0;32m     42\u001B[0m     llm \u001B[38;5;241m=\u001B[39m load_llm(config\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm_path\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_community\\llms\\loading.py:21\u001B[0m, in \u001B[0;36mload_llm_from_config\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m     18\u001B[0m type_to_cls_dict \u001B[38;5;241m=\u001B[39m get_type_to_cls_dict()\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m type_to_cls_dict:\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m LLM not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     23\u001B[0m llm_cls \u001B[38;5;241m=\u001B[39m type_to_cls_dict[config_type]()\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m llm_cls(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n",
      "\u001B[1;31mValueError\u001B[0m: Loading openai-chat LLM not supported"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "从磁盘加载链\n",
    "20240128 ValueError: Loading openai-chat LLM not supported\n",
    "\"\"\"\n",
    "from langchain.chains import load_chain\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "chain = load_chain(\"files/llm_chain.json\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:30.153648100Z",
     "start_time": "2024-01-28T14:34:30.122139700Z"
    }
   },
   "id": "7f2d7b1952702d49"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mQuestion: whats 2 + 2\n",
      " \n",
      "Answer: Let's think step by step.\u001B[0m\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAPIRemovedInV1\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhats 2 + 2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\base.py:162\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    163\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    164\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    165\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[0;32m    166\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    149\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[0;32m    150\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    151\u001B[0m     inputs,\n\u001B[0;32m    152\u001B[0m     name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[0;32m    153\u001B[0m )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    159\u001B[0m     )\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    116\u001B[0m         prompts,\n\u001B[0;32m    117\u001B[0m         stop,\n\u001B[0;32m    118\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    120\u001B[0m     )\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    124\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:525\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    518\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    519\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    523\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    524\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 525\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:698\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    682\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    683\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    684\u001B[0m         )\n\u001B[0;32m    685\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    686\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    687\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    696\u001B[0m         )\n\u001B[0;32m    697\u001B[0m     ]\n\u001B[1;32m--> 698\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[0;32m    699\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    700\u001B[0m     )\n\u001B[0;32m    701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:562\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[0;32m    561\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 562\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    563\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:549\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    541\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    545\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    546\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    547\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    548\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 549\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    550\u001B[0m                 prompts,\n\u001B[0;32m    551\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    552\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[0;32m    553\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    554\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    555\u001B[0m             )\n\u001B[0;32m    556\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    557\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    558\u001B[0m         )\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    560\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_community\\llms\\openai.py:1158\u001B[0m, in \u001B[0;36mOpenAIChat._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1156\u001B[0m messages, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_chat_params(prompts, stop)\n\u001B[0;32m   1157\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m-> 1158\u001B[0m full_response \u001B[38;5;241m=\u001B[39m completion_with_retry(\n\u001B[0;32m   1159\u001B[0m     \u001B[38;5;28mself\u001B[39m, messages\u001B[38;5;241m=\u001B[39mmessages, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m   1160\u001B[0m )\n\u001B[0;32m   1161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(full_response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m   1162\u001B[0m     full_response \u001B[38;5;241m=\u001B[39m full_response\u001B[38;5;241m.\u001B[39mdict()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain_community\\llms\\openai.py:115\u001B[0m, in \u001B[0;36mcompletion_with_retry\u001B[1;34m(llm, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    117\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(llm, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    119\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001B[0m, in \u001B[0;36mAPIRemovedInV1Proxy.__call__\u001B[1;34m(self, *_args, **_kwargs)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_args: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIRemovedInV1(symbol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol)\n",
      "\u001B[1;31mAPIRemovedInV1\u001B[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"whats 2 + 2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:34:39.838821Z",
     "start_time": "2024-01-28T14:34:39.767600300Z"
    }
   },
   "id": "73e4793b17a2ba8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "分别保存组件\n",
    "在上面的例子中，我们可以看到提示和llm配置信息保存在与整个链相同的json中。\n",
    "或者，我们可以将它们拆分并分别保存。这通常有助于使保存的组件更加模块化。为了做到这一点，我们只需要指定llm_path而不是llm组件，以及prompt_path而不是prompt组件。\n",
    "\"\"\"\n",
    "llm_chain.prompt.save(\"files/prompt.json\")\n",
    "llm_chain.llm.save(\"files/llm.json\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a430d33d7f6c15"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"memory\": None,\n",
    "    \"verbose\": True,\n",
    "    \"prompt_path\": \"prompt.json\",\n",
    "    \"llm_path\": \"llm.json\",\n",
    "    \"output_key\": \"text\",\n",
    "    \"_type\": \"llm_chain\"\n",
    "}\n",
    "import json\n",
    "with open(\"files/llm_chain_separate.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:35:50.292372200Z",
     "start_time": "2024-01-28T14:35:50.287379500Z"
    }
   },
   "id": "6b8f87c23723d1c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "然后我们可以以相同的方式加载它。\n",
    "\"\"\"\n",
    "chain = load_chain(\"llm_chain_separate.json\")\n",
    "chain.invoke(\"whats 2 + 2\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d7fd94b369d7498"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
