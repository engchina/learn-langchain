{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7031972a64445298",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76ab855bc398d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T03:41:06.388091Z",
     "start_time": "2024-01-06T03:41:05.830941300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OPENAI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# read local .env file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m _ \u001b[38;5;241m=\u001b[39m load_dotenv(find_dotenv())\n\u001b[0;32m---> 12\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_BASE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/learn-langchain/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OPENAI_API_KEY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "# read local .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48a0a825c54b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42549d96f2b5933b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T03:41:37.508787700Z",
     "start_time": "2024-01-06T03:41:07.959896200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `\"\"\"\n\n\ndef get_weather(city):\nimport requests\nfrom bs4 import BeautifulSoup\nurl = f\"https://www.xuanyi.com/{city}/weather.html\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"lxml\")\ntemp_str = soup.find(\"div\", {\"class\": \"temperature\"}).text\ntemp = float(temp_str[:-1])\nreturn temp\n\n\ndef get_weather_by_city():\nimport random\ncities = [\"上海\", \"宁波\", \"福建\"]\nwhile True:\ncity = random.choice(cities)\nif city == \"上海\":\ntemp = get_weather(\"上海\")\nelif city == \"宁波\":\ntemp = get_weather(\"宁波\")\nelse:\ntemp = get_weather(\"福建\")\nprint(f\"{city}的天气为：{temp}℃\")\n\n\ndef main():\nget_weather_by_city()\n\n\nif __name__ == \"__main__\":\nmain()`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1067\u001b[0m         intermediate_steps,\n\u001b[0;32m   1068\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1070\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:636\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m--> 636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\mrkl\\output_parser.py:63\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     66\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     67\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     71\u001b[0m ):\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `\"\"\"\n\n\ndef get_weather(city):\nimport requests\nfrom bs4 import BeautifulSoup\nurl = f\"https://www.xuanyi.com/{city}/weather.html\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"lxml\")\ntemp_str = soup.find(\"div\", {\"class\": \"temperature\"}).text\ntemp = float(temp_str[:-1])\nreturn temp\n\n\ndef get_weather_by_city():\nimport random\ncities = [\"上海\", \"宁波\", \"福建\"]\nwhile True:\ncity = random.choice(cities)\nif city == \"上海\":\ntemp = get_weather(\"上海\")\nelif city == \"宁波\":\ntemp = get_weather(\"宁波\")\nelse:\ntemp = get_weather(\"福建\")\nprint(f\"{city}的天气为：{temp}℃\")\n\n\ndef main():\nget_weather_by_city()\n\n\nif __name__ == \"__main__\":\nmain()`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(tools, chat, agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# agent.run(\"What will be the weather in Shanghai three days from now?\")\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m三天后上海的天气会怎么样？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    508\u001b[0m         _output_key\n\u001b[0;32m    509\u001b[0m     ]\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    513\u001b[0m         _output_key\n\u001b[0;32m    514\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1322\u001b[0m         )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         [\n\u001b[0;32m   1039\u001b[0m             a\n\u001b[0;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1042\u001b[0m                 color_mapping,\n\u001b[0;32m   1043\u001b[0m                 inputs,\n\u001b[0;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1045\u001b[0m                 run_manager,\n\u001b[0;32m   1046\u001b[0m             )\n\u001b[0;32m   1047\u001b[0m         ]\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         [\n\u001b[0;32m   1039\u001b[0m             a\n\u001b[0;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1042\u001b[0m                 color_mapping,\n\u001b[0;32m   1043\u001b[0m                 inputs,\n\u001b[0;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1045\u001b[0m                 run_manager,\n\u001b[0;32m   1046\u001b[0m             )\n\u001b[0;32m   1047\u001b[0m         ]\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\learn-langchain\\lib\\site-packages\\langchain\\agents\\agent.py:1077\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1077\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1078\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1079\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m     )\n\u001b[0;32m   1083\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `\"\"\"\n\n\ndef get_weather(city):\nimport requests\nfrom bs4 import BeautifulSoup\nurl = f\"https://www.xuanyi.com/{city}/weather.html\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"lxml\")\ntemp_str = soup.find(\"div\", {\"class\": \"temperature\"}).text\ntemp = float(temp_str[:-1])\nreturn temp\n\n\ndef get_weather_by_city():\nimport random\ncities = [\"上海\", \"宁波\", \"福建\"]\nwhile True:\ncity = random.choice(cities)\nif city == \"上海\":\ntemp = get_weather(\"上海\")\nelif city == \"宁波\":\ntemp = get_weather(\"宁波\")\nelse:\ntemp = get_weather(\"福建\")\nprint(f\"{city}的天气为：{temp}℃\")\n\n\ndef main():\nget_weather_by_city()\n\n\nif __name__ == \"__main__\":\nmain()`"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=chat)\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.run(\"What will be the weather in Shanghai three days from now?\")\n",
    "agent.run(\"三天后上海的天气会怎么样？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c5a02554e0088",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 记忆组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7ec6d0c6feb2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T11:03:14.642736400Z",
     "start_time": "2023-12-24T11:02:23.520097500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！很高兴认识你。作为一个AI助手，我可以回答你的问题、提供信息和帮助解决问题。请随时告诉我需要什么帮助。\\n\\n你说你是程序员，那么你最常用的编程语言是哪个？我知道很多程序员都喜欢使用Python或JavaScript，因为它们有丰富的库和框架支持。但是，根据你的需求和偏好，也可能会选择其他语言。\\n\\n如果你遇到任何问题或困难，请告诉我，我将尽力提供帮助。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "The following is a friendly conversation between a human and an AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.        \n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"你好，我是程序员\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06eb55a7300c32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LangChain 表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5719c54061d3a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T11:03:26.705380400Z",
     "start_time": "2023-12-24T11:03:14.642736400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one: Why don't bears use email? Because they can't handle too many attachments! How was that?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"bear\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a6a90d37d1c543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T11:03:40.040113400Z",
     "start_time": "2023-12-24T11:03:26.705380400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, why not? Here's one for you: Why do bears hate shoes so much? Because they like to run around in their bear feet! Is that the kind of joke you were looking for?\"\n"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"joke\",\n",
    "        \"description\": \"tell me a joke\",\n",
    "        \"parameters\": {\n",
    "            \"topic\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"setup\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The setup for the joke\"\n",
    "                    },\n",
    "                    \"punchline\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The punchline for the joke\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"setup\", \"punchline\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n",
    "response = chain.invoke({\"topic\": \"bear\"}, config={})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b77bc6028cda48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 多功能组合链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52fe5e72f8df0a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T11:04:00.391847200Z",
     "start_time": "2023-12-24T11:03:40.042535200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama was born in Honolulu, Hawaii.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这个城市位于夏威夷州，美国。中文：这个城市位于夏威夷州，美国。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "response = chain1.invoke({\"person\": \"obama\"})\n",
    "print(response)\n",
    "chain2 = (\n",
    "        {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "        | prompt2\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"Chinese\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a57f64b3e912e48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T11:10:38.579365700Z",
     "start_time": "2023-12-24T11:09:59.901288500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"what is the color of The name of the fruit that is this color is not something I can determine as it's a random color and not associated with any specific fruit. However, some fruits that are shades of purple include eggplant, acai berries, blackberries, and plums. and The country with a flag closest to the color you provided (#8A2BE2) is Dominica. However, please note that no country has an exact match for this specific shade of purple.\")])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"generate a random color\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what is a fruit of color: {color}\")\n",
    "prompt3 = ChatPromptTemplate.from_template(\"what is countries flag that has the color: {color}\")\n",
    "prompt4 = ChatPromptTemplate.from_template(\"what is the color of {fruit} and {country}\")\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "chain2 = RunnableMap(color={\"color\": chain1}) | {\"fruit\": prompt2 | model | StrOutputParser(),\n",
    "                                                 \"country\": prompt3 | model | StrOutputParser(), } | prompt4\n",
    "chain2.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbee20e7055bed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
